{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Generative Text Start with an Amazon ASIN that we\u2019re going to optimize a Product Title for (GT-2) Extract from its existing text-based listing content all possible keyphrases that describe the product a. Sources to analyze: the listing itself on Amazon (scraping) and what\u2019s pulled into our sp_all_listings_master table in BigQuery for the product b. When scraping, we\u2019re looking to scrape: i. Product Title: id=\u201cproductTitle\u201d ii. Key Product Features: id=\u201cfeature-bullets\u201d and the a-list-item class underneath iii. Alt text on A+ Content Image: id=\u201caplus\u201d --> class=\u201caplus-module\u201d and then img alt underneath that meta description c. In our BQ table, the following fields should have some information: i. item_name = Product Title ii. item_description = Product Decription (GT-3) Based on all keyphrases extracted, we score each one and rank them based on seeming relevance to the original text to develop a shortlist of keyphrases (GT-4) We then take the keyphrases (starting with the most relevant ones) and query any of the ba_search_terms tables in BQ (like sp_ba_search_terms_by_month or sp_ba_search_terms_by_quarter) to see what search terms contain those keyphrases, sorted by SFR in ascending order. Low SFR means more traffic (GT-5) Taking the highly trafficked (low SFR) and calculate their relevancy to the original product listing to see which is the best fit and which is the focal keyword (GT-1) Scrape the Amazon listing\u2019s reviews, put the raw reviews scraped into a table in BQ (not yet created, we need to build one), and extract core concepts from them along with frequency and avg rating (GT-9) Score each review\u2019s concept based on its relevancy to the original listing text (GT-10) Take the focal keyword and search it in Amazon to scrape the top Non-Sponsored ASINs that are our organic SEO competitors using this [URL syntax:] (https://www.amazon.com/s?k=QUERY) for other marketplaces we query the other domains, i.e. for our [UK client] (https://amazon.co.uk/s?k=QUERY) (GT-11) Scrape their listing contents and confirm that they are relevant to our listing (GT-12) For those relevant, scrape the reviews and load them into BQ, then extract their concepts and store them in BQ (GT-18) Relevancy Score: Competitor Review Concepts (GT-17) Aggregate the following to be built into a prompt for GPT a. Low SFR (high volume) highly relevant terms from sp_ba_search_terms. Anything that\u2019s under SFR 1m is considered good volume. We are looking for breadth of keyword descriptors that we can use in our title b.Review concepts from our own reviews that are highly mentioned and highly ranked (avg rating assigned to them) c. Review concepts from other competitor reviews that are highly mentioned and highly ranked and relevant to ours (GT-14) Send the prompt into GPT for different title variations to be output. Maybe take the best 5 out of 25 different generated options Zoom Meetings Zoom meetings: [1/20/23:] (https://drive.google.com/file/d/1enPFl42ZBVjNu86x_yY2qpJzUY1-QmuZ/view?usp=share_link) [1/27/23:] (https://drive.google.com/file/d/1XTJ5DKan_VeGL5r6XzU4qAhEVuOn1ch8/view?usp=sharing) [2/3/23:] (https://drive.google.com/file/d/1Juj7kdcSiXHJ7Nr2UMu4u_JDcpv7MmoT/view?usp=sharing)","title":"Home"},{"location":"#generative-text","text":"Start with an Amazon ASIN that we\u2019re going to optimize a Product Title for (GT-2) Extract from its existing text-based listing content all possible keyphrases that describe the product a. Sources to analyze: the listing itself on Amazon (scraping) and what\u2019s pulled into our sp_all_listings_master table in BigQuery for the product b. When scraping, we\u2019re looking to scrape: i. Product Title: id=\u201cproductTitle\u201d ii. Key Product Features: id=\u201cfeature-bullets\u201d and the a-list-item class underneath iii. Alt text on A+ Content Image: id=\u201caplus\u201d --> class=\u201caplus-module\u201d and then img alt underneath that meta description c. In our BQ table, the following fields should have some information: i. item_name = Product Title ii. item_description = Product Decription (GT-3) Based on all keyphrases extracted, we score each one and rank them based on seeming relevance to the original text to develop a shortlist of keyphrases (GT-4) We then take the keyphrases (starting with the most relevant ones) and query any of the ba_search_terms tables in BQ (like sp_ba_search_terms_by_month or sp_ba_search_terms_by_quarter) to see what search terms contain those keyphrases, sorted by SFR in ascending order. Low SFR means more traffic (GT-5) Taking the highly trafficked (low SFR) and calculate their relevancy to the original product listing to see which is the best fit and which is the focal keyword (GT-1) Scrape the Amazon listing\u2019s reviews, put the raw reviews scraped into a table in BQ (not yet created, we need to build one), and extract core concepts from them along with frequency and avg rating (GT-9) Score each review\u2019s concept based on its relevancy to the original listing text (GT-10) Take the focal keyword and search it in Amazon to scrape the top Non-Sponsored ASINs that are our organic SEO competitors using this [URL syntax:] (https://www.amazon.com/s?k=QUERY) for other marketplaces we query the other domains, i.e. for our [UK client] (https://amazon.co.uk/s?k=QUERY) (GT-11) Scrape their listing contents and confirm that they are relevant to our listing (GT-12) For those relevant, scrape the reviews and load them into BQ, then extract their concepts and store them in BQ (GT-18) Relevancy Score: Competitor Review Concepts (GT-17) Aggregate the following to be built into a prompt for GPT a. Low SFR (high volume) highly relevant terms from sp_ba_search_terms. Anything that\u2019s under SFR 1m is considered good volume. We are looking for breadth of keyword descriptors that we can use in our title b.Review concepts from our own reviews that are highly mentioned and highly ranked (avg rating assigned to them) c. Review concepts from other competitor reviews that are highly mentioned and highly ranked and relevant to ours (GT-14) Send the prompt into GPT for different title variations to be output. Maybe take the best 5 out of 25 different generated options","title":"Generative Text"},{"location":"#zoom-meetings","text":"Zoom meetings: [1/20/23:] (https://drive.google.com/file/d/1enPFl42ZBVjNu86x_yY2qpJzUY1-QmuZ/view?usp=share_link) [1/27/23:] (https://drive.google.com/file/d/1XTJ5DKan_VeGL5r6XzU4qAhEVuOn1ch8/view?usp=sharing) [2/3/23:] (https://drive.google.com/file/d/1Juj7kdcSiXHJ7Nr2UMu4u_JDcpv7MmoT/view?usp=sharing)","title":"Zoom Meetings"},{"location":"GT-10/","text":"Focal Keyword Search (GT-10) Overview This Python script searches Amazon for given search keywords and extracts the ASINs of non-sponsored products from the first 4 pages of search results. The script uses the Scraper API to scrape the data and BeautifulSoup library to parse the HTML content. Table of Contents Focal Keyword Search (GT-10) Current endpoint: Requirements Flow Description Input Example input: Output Example output: Amazon Regions How to use it To deploy this locally: Current endpoint: gt-10-focal-keyword-search: https://gt-10-focal-keyword-search-kknzgohiuq-uc.a.run.app Requirements To run the script, the following requirements should be met: Python environment with the necessary libraries installed ( os , functions_framework , bs4 , scraper_api , logging ). The SCRAPER_API_KEY environment variable should be set with a valid API key for the Scraper API service. The script should be deployed and configured to handle HTTP requests. Flow Description The main function takes search keywords and Amazon region as input. It validates the input and calls the search_amazon() function with the search keywords and region. The search_amazon() function searches Amazon for the given keywords and extracts the ASINs of non-sponsored products from the first 4 pages of search results. The parse_data() function is used to parse the HTML content and extract the ASINs. The list of extracted ASINs is returned along with the search keywords. Input search_keywords : A list of search terms to search on Amazon. region : (Optional) Amazon country site to search. Default is 'us'. Example input: { \"search_keywords\": [\"laptop\", \"smartphone\"], \"region\": \"us\" } Output scraped_asins : A list of extracted ASINs. search_keywords : The original list of search keywords. Example output: { \"scraped_asins\": [ \"B08N5WRWNW\", \"B08N5LNQCX\", \"B08N5M7S6K\", \"B08N5M8Z4K\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\" ], \"search_keywords\": [\"laptop\", \"smartphone\"] } Amazon Regions The script supports the following Amazon regions: us: amazon.com uk: amazon.co.uk ca: amazon.ca de: amazon.de es: amazon.es fr: amazon.fr it: amazon.it jp: amazon.co.jp in: amazon.in cn: amazon.cn sg: amazon.com.sg mx: amazon.com.mx ae: amazon.ae br: amazon.com.br nl: amazon.nl au: amazon.com.au tr: amazon.com.tr sa: amazon.sa se: amazon.se pl: amazon.pl How to use it To deploy this locally: Create a Python virtual environment (Python 3.11.1) and activate it bash python -m venv venv source venv/bin/activate Activate the virtual environment using source venv/bin/activate Install the requirements file bash pip install -r requirements-dev.txt Run the development server using functions-framework --target main --debug --port 8080 Access the app home page at http://127.0.0.1:8080/","title":"GT-10"},{"location":"GT-10/#focal-keyword-search-gt-10","text":"","title":"Focal Keyword Search (GT-10)"},{"location":"GT-10/#overview","text":"This Python script searches Amazon for given search keywords and extracts the ASINs of non-sponsored products from the first 4 pages of search results. The script uses the Scraper API to scrape the data and BeautifulSoup library to parse the HTML content.","title":"Overview"},{"location":"GT-10/#table-of-contents","text":"Focal Keyword Search (GT-10) Current endpoint: Requirements Flow Description Input Example input: Output Example output: Amazon Regions How to use it To deploy this locally:","title":"Table of Contents"},{"location":"GT-10/#current-endpoint","text":"gt-10-focal-keyword-search: https://gt-10-focal-keyword-search-kknzgohiuq-uc.a.run.app","title":"Current endpoint:"},{"location":"GT-10/#requirements","text":"To run the script, the following requirements should be met: Python environment with the necessary libraries installed ( os , functions_framework , bs4 , scraper_api , logging ). The SCRAPER_API_KEY environment variable should be set with a valid API key for the Scraper API service. The script should be deployed and configured to handle HTTP requests.","title":"Requirements"},{"location":"GT-10/#flow-description","text":"The main function takes search keywords and Amazon region as input. It validates the input and calls the search_amazon() function with the search keywords and region. The search_amazon() function searches Amazon for the given keywords and extracts the ASINs of non-sponsored products from the first 4 pages of search results. The parse_data() function is used to parse the HTML content and extract the ASINs. The list of extracted ASINs is returned along with the search keywords.","title":"Flow Description"},{"location":"GT-10/#input","text":"search_keywords : A list of search terms to search on Amazon. region : (Optional) Amazon country site to search. Default is 'us'.","title":"Input"},{"location":"GT-10/#example-input","text":"{ \"search_keywords\": [\"laptop\", \"smartphone\"], \"region\": \"us\" }","title":"Example input:"},{"location":"GT-10/#output","text":"scraped_asins : A list of extracted ASINs. search_keywords : The original list of search keywords.","title":"Output"},{"location":"GT-10/#example-output","text":"{ \"scraped_asins\": [ \"B08N5WRWNW\", \"B08N5LNQCX\", \"B08N5M7S6K\", \"B08N5M8Z4K\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\", \"B08N5LFLC3\" ], \"search_keywords\": [\"laptop\", \"smartphone\"] }","title":"Example output:"},{"location":"GT-10/#amazon-regions","text":"The script supports the following Amazon regions: us: amazon.com uk: amazon.co.uk ca: amazon.ca de: amazon.de es: amazon.es fr: amazon.fr it: amazon.it jp: amazon.co.jp in: amazon.in cn: amazon.cn sg: amazon.com.sg mx: amazon.com.mx ae: amazon.ae br: amazon.com.br nl: amazon.nl au: amazon.com.au tr: amazon.com.tr sa: amazon.sa se: amazon.se pl: amazon.pl","title":"Amazon Regions"},{"location":"GT-10/#how-to-use-it","text":"","title":"How to use it"},{"location":"GT-10/#to-deploy-this-locally","text":"Create a Python virtual environment (Python 3.11.1) and activate it bash python -m venv venv source venv/bin/activate Activate the virtual environment using source venv/bin/activate Install the requirements file bash pip install -r requirements-dev.txt Run the development server using functions-framework --target main --debug --port 8080 Access the app home page at http://127.0.0.1:8080/","title":"To deploy this locally:"},{"location":"GT-11/","text":"Listings Relevancy (GT-11) Overview This script is a relevancy checker that uses OpenAI's GPT-3.5-turbo to determine the relevancy between two product listings on a scale of 0 to 1 (0 being totally irrelevant and 1 being most relevant). It takes client and competitors ASINs (Amazon Standard Identification Number) as input and returns the relevancy score between them. Table of Contents Listings Relevancy (GT-11) Current endpoint: Requirements Flow Description Input Output How to use it To deploy this locally: Current endpoint: gt-11-listings relevancy: https://gt-11-listings-relevancy-kknzgohiuq-uc.a.run.app Requirements To run the script, the following requirements should be met: Python environment with the necessary libraries installed ( os , functions_framework , bs4 , scraper_api , logging , openai ). The OPENAI_KEY environment variable should be set with a valid API key for the OpenAI service. The script should be deployed and configured to handle HTTP requests. Flow Description The script receives an HTTP request with the required inputs ( our_product_asin and relevant_products_asin ). It calls the load_listing_data function for each ASIN to load the product data from an external API. The load_listing_data_bq function is called to load the product data from a BigQuery table. The get_listings_relevancy_openai function is called to determine the relevancy score between the product listings using OpenAI's GPT-3.5-turbo. The final result is returned as a JSON object containing the relevancy scores for each ASIN in the relevant_products_asin list. Input The input is a JSON object containing two keys: our_product_asin : The ASIN of the product we want to compare (e.g., B0874XN4D8). relevant_products_asin : A list of ASINs of the relevant products (e.g., ['B09VLK9W3S', 'B09VSR2ZHD']). Example input: { \"our_product_asin\": \"B0874XN4D8\", \"relevant_products_asin\": [\"B09VLK9W3S\", \"B09VSR2ZHD\"] } Output The output is a JSON object containing the relevancy scores for each ASIN in the relevant_products_asin list. The keys are the ASINs, and the values are the relevancy scores. Example output: { \"results\": { \"B09VLK9W3S\": 0.8, \"B09VSR2ZHD\": 0.6 }, \"our_product_asin\": \"B0874XN4D8\" } In this example, the relevancy score between our product (B0874XN4D8) and B09VLK9W3S is 0.8, and the relevancy score between our product (B0874XN4D8) and B09VSR2ZHD is 0.6. How to use it To deploy this locally: To deploy this locally: Create a Python virtual environment (Python 3.11.1) and activate it bash python -m venv venv source venv/bin/activate Activate the virtual environment using source venv/bin/activate Install the requirements file bash pip install -r requirements-dev.txt Run the development server using functions-framework --target main --debug --port 8080 Access the app home page at http://127.0.0.1:8080/","title":"GT-11"},{"location":"GT-11/#listings-relevancy-gt-11","text":"","title":"Listings Relevancy (GT-11)"},{"location":"GT-11/#overview","text":"This script is a relevancy checker that uses OpenAI's GPT-3.5-turbo to determine the relevancy between two product listings on a scale of 0 to 1 (0 being totally irrelevant and 1 being most relevant). It takes client and competitors ASINs (Amazon Standard Identification Number) as input and returns the relevancy score between them.","title":"Overview"},{"location":"GT-11/#table-of-contents","text":"Listings Relevancy (GT-11) Current endpoint: Requirements Flow Description Input Output How to use it To deploy this locally:","title":"Table of Contents"},{"location":"GT-11/#current-endpoint","text":"gt-11-listings relevancy: https://gt-11-listings-relevancy-kknzgohiuq-uc.a.run.app","title":"Current endpoint:"},{"location":"GT-11/#requirements","text":"To run the script, the following requirements should be met: Python environment with the necessary libraries installed ( os , functions_framework , bs4 , scraper_api , logging , openai ). The OPENAI_KEY environment variable should be set with a valid API key for the OpenAI service. The script should be deployed and configured to handle HTTP requests.","title":"Requirements"},{"location":"GT-11/#flow-description","text":"The script receives an HTTP request with the required inputs ( our_product_asin and relevant_products_asin ). It calls the load_listing_data function for each ASIN to load the product data from an external API. The load_listing_data_bq function is called to load the product data from a BigQuery table. The get_listings_relevancy_openai function is called to determine the relevancy score between the product listings using OpenAI's GPT-3.5-turbo. The final result is returned as a JSON object containing the relevancy scores for each ASIN in the relevant_products_asin list.","title":"Flow Description"},{"location":"GT-11/#input","text":"The input is a JSON object containing two keys: our_product_asin : The ASIN of the product we want to compare (e.g., B0874XN4D8). relevant_products_asin : A list of ASINs of the relevant products (e.g., ['B09VLK9W3S', 'B09VSR2ZHD']).","title":"Input"},{"location":"GT-11/#example-input","text":"{ \"our_product_asin\": \"B0874XN4D8\", \"relevant_products_asin\": [\"B09VLK9W3S\", \"B09VSR2ZHD\"] }","title":"Example input:"},{"location":"GT-11/#output","text":"The output is a JSON object containing the relevancy scores for each ASIN in the relevant_products_asin list. The keys are the ASINs, and the values are the relevancy scores.","title":"Output"},{"location":"GT-11/#example-output","text":"{ \"results\": { \"B09VLK9W3S\": 0.8, \"B09VSR2ZHD\": 0.6 }, \"our_product_asin\": \"B0874XN4D8\" } In this example, the relevancy score between our product (B0874XN4D8) and B09VLK9W3S is 0.8, and the relevancy score between our product (B0874XN4D8) and B09VSR2ZHD is 0.6.","title":"Example output:"},{"location":"GT-11/#how-to-use-it","text":"","title":"How to use it"},{"location":"GT-11/#to-deploy-this-locally","text":"To deploy this locally: Create a Python virtual environment (Python 3.11.1) and activate it bash python -m venv venv source venv/bin/activate Activate the virtual environment using source venv/bin/activate Install the requirements file bash pip install -r requirements-dev.txt Run the development server using functions-framework --target main --debug --port 8080 Access the app home page at http://127.0.0.1:8080/","title":"To deploy this locally:"},{"location":"GT-2/","text":"GT14 : Titles and hypotheses Generation Overview This Python script is designed to generate titles and hypotheses for a given product listing based on input data such as search terms, reviews concepts, and competitors' reviews concepts. It uses the FastAPI framework, OpenAI API and Secret Manager. The script defines a FastAPI application and an API endpoint to handle incoming requests and return the generated titles and hypotheses. Test Endpoint : https://gt14-j3sh6qxa4a-uc.a.run.app Production Endpoint : Not deployed yet GT14 : Titles and hypotheses Generation Requirements Python Version Libraries External Tools How to use it Try the deployed API Running Locally Deploying to Cloud Run Functions and Classes FastAPI and APIRouter Main Function Purpose: Input Parameters: Example: Return Values Example Helper Functions Schemas Utils Project Structure Testing Contributing Requirements This Python script generates titles and hypotheses using FastAPI, OpenAI API, and Google Secret Manager. Below are the requirements and dependencies needed to run the script. Python Version Python 3.7 or higher Libraries fastapi==0.68.1 uvicorn==0.15.0 gunicorn==20.1.0 httpx==0.22.0 pydantic==1.8.2 google-cloud-secret-manager==2.7.1 openai==0.27.0 External Tools Google Cloud Secret Manager (to store the OpenAI API key) How to use it Try the deployed API You can visit the deployed version of the API by adding /docs at the end of the endpoint URL. This will provide access to the API documentation and an interface to try the API. Visit the documentation or try the API using this URL : https://gt14-j3sh6qxa4a-uc.a.run.app/docs Running Locally To run this API locally, follow these steps: Create and activate a virtual environment. Install the required libraries by running pip install -r requirements.txt. Launch the app using the command python -m uvicorn app:app --reload. Access the API at http://127.0.0.1:8000/. Note that you will need to have access to the Secret Manager with your Google Account since the OpenAI API key is stored there. If you do not have access, replace openai_key in the file app/main.py with your own OpenAI key. Deploying to Cloud Run The API is currently deployed on Cloud Run. To deploy it to your own project, follow these steps: Login to the project account using the command gcloud auth login . Set the project using the command gcloud config set project <project-id> . Build the Docker image using the command docker build -t gcr.io/<project-id>/gt14 . . Push the image to Google Container Registry using the command docker push gcr.io/<project-id>/gt14 . You may need to run the command gcloud auth configure-docker . Deploy the image to Cloud Run using the command gcloud run deploy gt14 --image gcr.io/<project-id>/gt14 --platform managed --region us-central1 --allow-unauthenticated Functions and Classes FastAPI and APIRouter FastAPI() : Creates a new FastAPI application. APIRouter() : Creates a new APIRouter instance for handling routes in a modular way. Main Function generate_title_and_hipothesis(request: Request, asins_input: TitlesInput, number_titles: int = 5) -> TitlesOutput : This function is responsible for generating a list of titles and hypotheses for a given product listing, a list of search terms, reviews concepts, and competitors reviews concepts. Purpose: To generate a list of titles and hypotheses using OpenAI API. Input Parameters: asins_input (TitlesInput): search_terms (List[str]): A list of search terms related to the product. reviews_concepts (List[str]): A list of concepts derived from product reviews. competitors_reviews_concepts (List[str]): A list of concepts derived from competitor's product reviews. product_listing (str): The product listing text. number_titles (int, optional): Number of titles to generate. Defaults to 5. Example: { \"asins_input\": { \"search_terms\": [\"term1\", \"term2\", \"term3\"], \"reviews_concepts\": [\"concept1\", \"concept2\", \"concept3\"], \"competitors_reviews_concepts\": [\"competitor_concept1\", \"competitor_concept2\", \"competitor_concept3\"], \"product_listing\": \"Sample product listing text\" }, \"number_titles\": 5 } Return Values titles (List[str]): A list of generated titles. hypotheses (List[Dict[str, str]]): A list of dictionaries, each containing a generated title and its corresponding hypothesis. Example { \"titles\": [\"Generated Title 1\", \"Generated Title 2\", \"Generated Title 3\", \"Generated Title 4\", \"Generated Title 5\"], \"hypotheses\": [ { \"title\": \"Generated Title 1\", \"hypothesis\": \"Generated Hypothesis for Title 1\" }, { \"title\": \"Generated Title 2\", \"hypothesis\": \"Generated Hypothesis for Title 2\" }, { \"title\": \"Generated Title 3\", \"hypothesis\": \"Generated Hypothesis for Title 3\" }, { \"title\": \"Generated Title 4\", \"hypothesis\": \"Generated Hypothesis for Title 4\" }, { \"title\": \"Generated Title 5\", \"hypothesis\": \"Generated Hypothesis for Title 5\" } ] } Helper Functions generate_titles() : Generates titles using OpenAI API based on the input parameters. generate_hypotheses() : Generates hypotheses using OpenAI API based on the input titles and product listing. Schemas TitlesInput : A Pydantic model defining the input data for generating titles and hypotheses. TitlesOutput : A Pydantic model defining the output data containing the generated titles and hypotheses. Utils logger : A utility for logging messages. get_secret() : A utility function to get the OpenAI API key from Secret Manager. Project Structure At the root of the project, you will find the following files : * requirements.txt : lists all the required libraries. * README.md : provides information on how to use the API. * main.py : defines the host and port for FastAPI. * Dockerfile : specifies the Docker configuration. The /app directory contains the following files: * schemas : a folder that contains all the schemas for the API. * tests : a folder that contains all the tests for the API. * utils : a folder that contains scripts such as logger , exceptions and secret_manager . * main.py : a script where all the main logic is defined. * openai.py : a script where all the methods related to the OpenAI API are defined. * README.md : a detailed description of the app, including its purpose, functionality, and any unique features. Testing Contributing","title":"GT-2"},{"location":"GT-2/#gt14-titles-and-hypotheses-generation","text":"","title":"GT14 : Titles and hypotheses Generation"},{"location":"GT-2/#overview","text":"This Python script is designed to generate titles and hypotheses for a given product listing based on input data such as search terms, reviews concepts, and competitors' reviews concepts. It uses the FastAPI framework, OpenAI API and Secret Manager. The script defines a FastAPI application and an API endpoint to handle incoming requests and return the generated titles and hypotheses. Test Endpoint : https://gt14-j3sh6qxa4a-uc.a.run.app Production Endpoint : Not deployed yet GT14 : Titles and hypotheses Generation Requirements Python Version Libraries External Tools How to use it Try the deployed API Running Locally Deploying to Cloud Run Functions and Classes FastAPI and APIRouter Main Function Purpose: Input Parameters: Example: Return Values Example Helper Functions Schemas Utils Project Structure Testing Contributing","title":"Overview"},{"location":"GT-2/#requirements","text":"This Python script generates titles and hypotheses using FastAPI, OpenAI API, and Google Secret Manager. Below are the requirements and dependencies needed to run the script.","title":"Requirements"},{"location":"GT-2/#python-version","text":"Python 3.7 or higher","title":"Python Version"},{"location":"GT-2/#libraries","text":"fastapi==0.68.1 uvicorn==0.15.0 gunicorn==20.1.0 httpx==0.22.0 pydantic==1.8.2 google-cloud-secret-manager==2.7.1 openai==0.27.0","title":"Libraries"},{"location":"GT-2/#external-tools","text":"Google Cloud Secret Manager (to store the OpenAI API key)","title":"External Tools"},{"location":"GT-2/#how-to-use-it","text":"","title":"How to use it"},{"location":"GT-2/#try-the-deployed-api","text":"You can visit the deployed version of the API by adding /docs at the end of the endpoint URL. This will provide access to the API documentation and an interface to try the API. Visit the documentation or try the API using this URL : https://gt14-j3sh6qxa4a-uc.a.run.app/docs","title":"Try the deployed API"},{"location":"GT-2/#running-locally","text":"To run this API locally, follow these steps: Create and activate a virtual environment. Install the required libraries by running pip install -r requirements.txt. Launch the app using the command python -m uvicorn app:app --reload. Access the API at http://127.0.0.1:8000/. Note that you will need to have access to the Secret Manager with your Google Account since the OpenAI API key is stored there. If you do not have access, replace openai_key in the file app/main.py with your own OpenAI key.","title":"Running Locally"},{"location":"GT-2/#deploying-to-cloud-run","text":"The API is currently deployed on Cloud Run. To deploy it to your own project, follow these steps: Login to the project account using the command gcloud auth login . Set the project using the command gcloud config set project <project-id> . Build the Docker image using the command docker build -t gcr.io/<project-id>/gt14 . . Push the image to Google Container Registry using the command docker push gcr.io/<project-id>/gt14 . You may need to run the command gcloud auth configure-docker . Deploy the image to Cloud Run using the command gcloud run deploy gt14 --image gcr.io/<project-id>/gt14 --platform managed --region us-central1 --allow-unauthenticated","title":"Deploying to Cloud Run"},{"location":"GT-2/#functions-and-classes","text":"","title":"Functions and Classes"},{"location":"GT-2/#fastapi-and-apirouter","text":"FastAPI() : Creates a new FastAPI application. APIRouter() : Creates a new APIRouter instance for handling routes in a modular way.","title":"FastAPI and APIRouter"},{"location":"GT-2/#main-function","text":"generate_title_and_hipothesis(request: Request, asins_input: TitlesInput, number_titles: int = 5) -> TitlesOutput : This function is responsible for generating a list of titles and hypotheses for a given product listing, a list of search terms, reviews concepts, and competitors reviews concepts.","title":"Main Function"},{"location":"GT-2/#purpose","text":"To generate a list of titles and hypotheses using OpenAI API.","title":"Purpose:"},{"location":"GT-2/#input-parameters","text":"asins_input (TitlesInput): search_terms (List[str]): A list of search terms related to the product. reviews_concepts (List[str]): A list of concepts derived from product reviews. competitors_reviews_concepts (List[str]): A list of concepts derived from competitor's product reviews. product_listing (str): The product listing text. number_titles (int, optional): Number of titles to generate. Defaults to 5.","title":"Input Parameters:"},{"location":"GT-2/#example","text":"{ \"asins_input\": { \"search_terms\": [\"term1\", \"term2\", \"term3\"], \"reviews_concepts\": [\"concept1\", \"concept2\", \"concept3\"], \"competitors_reviews_concepts\": [\"competitor_concept1\", \"competitor_concept2\", \"competitor_concept3\"], \"product_listing\": \"Sample product listing text\" }, \"number_titles\": 5 }","title":"Example:"},{"location":"GT-2/#return-values","text":"titles (List[str]): A list of generated titles. hypotheses (List[Dict[str, str]]): A list of dictionaries, each containing a generated title and its corresponding hypothesis.","title":"Return Values"},{"location":"GT-2/#example_1","text":"{ \"titles\": [\"Generated Title 1\", \"Generated Title 2\", \"Generated Title 3\", \"Generated Title 4\", \"Generated Title 5\"], \"hypotheses\": [ { \"title\": \"Generated Title 1\", \"hypothesis\": \"Generated Hypothesis for Title 1\" }, { \"title\": \"Generated Title 2\", \"hypothesis\": \"Generated Hypothesis for Title 2\" }, { \"title\": \"Generated Title 3\", \"hypothesis\": \"Generated Hypothesis for Title 3\" }, { \"title\": \"Generated Title 4\", \"hypothesis\": \"Generated Hypothesis for Title 4\" }, { \"title\": \"Generated Title 5\", \"hypothesis\": \"Generated Hypothesis for Title 5\" } ] }","title":"Example"},{"location":"GT-2/#helper-functions","text":"generate_titles() : Generates titles using OpenAI API based on the input parameters. generate_hypotheses() : Generates hypotheses using OpenAI API based on the input titles and product listing.","title":"Helper Functions"},{"location":"GT-2/#schemas","text":"TitlesInput : A Pydantic model defining the input data for generating titles and hypotheses. TitlesOutput : A Pydantic model defining the output data containing the generated titles and hypotheses.","title":"Schemas"},{"location":"GT-2/#utils","text":"logger : A utility for logging messages. get_secret() : A utility function to get the OpenAI API key from Secret Manager.","title":"Utils"},{"location":"GT-2/#project-structure","text":"At the root of the project, you will find the following files : * requirements.txt : lists all the required libraries. * README.md : provides information on how to use the API. * main.py : defines the host and port for FastAPI. * Dockerfile : specifies the Docker configuration. The /app directory contains the following files: * schemas : a folder that contains all the schemas for the API. * tests : a folder that contains all the tests for the API. * utils : a folder that contains scripts such as logger , exceptions and secret_manager . * main.py : a script where all the main logic is defined. * openai.py : a script where all the methods related to the OpenAI API are defined. * README.md : a detailed description of the app, including its purpose, functionality, and any unique features.","title":"Project Structure"},{"location":"GT-2/#testing","text":"","title":"Testing"},{"location":"GT-2/#contributing","text":"","title":"Contributing"},{"location":"GT-26/","text":"GT-26 Test text for gt-26","title":"GT-26"},{"location":"GT-26/#gt-26","text":"Test text for gt-26","title":"GT-26"},{"location":"GT-4/","text":"GT-4 Test text for gt-4","title":"GT-4"},{"location":"GT-4/#gt-4","text":"Test text for gt-4","title":"GT-4"},{"location":"gt-2/","text":"GT-2 Extract from its existing text-based listing content all possible keyphrases that describe the product a. Sources to analyze: the listing itself on Amazon (scraping) and what\u2019s pulled into our sp_all_listings_master table in BigQuery for the product b. When scraping, we\u2019re looking to scrape: i. Product Title: id=\u201cproductTitle\u201d ii. Key Product Features: id=\u201cfeature-bullets\u201d and the a-list-item class underneath iii. Alt text on A+ Content Image: id=\u201caplus\u201d --> class=\u201caplus-module\u201d and then img alt underneath that meta description c. In our BQ table, the following fields should have some information: i. item_name = Product Title ii. item_description = Product Decription","title":"GT-2"},{"location":"gt-2/#gt-2","text":"Extract from its existing text-based listing content all possible keyphrases that describe the product a. Sources to analyze: the listing itself on Amazon (scraping) and what\u2019s pulled into our sp_all_listings_master table in BigQuery for the product b. When scraping, we\u2019re looking to scrape: i. Product Title: id=\u201cproductTitle\u201d ii. Key Product Features: id=\u201cfeature-bullets\u201d and the a-list-item class underneath iii. Alt text on A+ Content Image: id=\u201caplus\u201d --> class=\u201caplus-module\u201d and then img alt underneath that meta description c. In our BQ table, the following fields should have some information: i. item_name = Product Title ii. item_description = Product Decription","title":"GT-2"}]}